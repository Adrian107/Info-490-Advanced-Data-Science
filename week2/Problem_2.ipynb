{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "75e65b1779f47a8a9eeb51fef2e423de",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Week 2 Problem 2\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says YOUR CODE HERE. Do not write your answer in anywhere else other than where it says YOUR CODE HERE. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select Kernel, and restart the kernel and run all cells (Restart & Run all).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select File → Save and CheckPoint)\n",
    "\n",
    "5. When you are ready to submit your assignment, go to Dashboard → Assignments and click the Submit button. Your work is not submitted until you click Submit.\n",
    "\n",
    "6. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded.\n",
    "\n",
    "7. If your code does not pass the unit tests, it will not pass the autograder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "86144825be6114606d50612d90b0b66e",
     "grade": false,
     "grade_id": "due_date",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Due Date: 6 PM, January 29, 2018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1d87f1157985fa4c035b1239703c38c5",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nose.tools import assert_false, assert_equal, assert_almost_equal, assert_true\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from nose.tools import assert_equal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "# Set up Notebook\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ee0056a9e4f1e898e716896768f7b127",
     "grade": false,
     "grade_id": "reading_titanic",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Reading in the Titanic Data Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f74f7478789670bd304d24c46b3effb9",
     "grade": false,
     "grade_id": "titanic_info",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "This data set provides information on the fate of passengers on the fatal maiden voyage of the ocean liner Titanic, summarized according to economic status (class), sex, age and survival.\n",
    "\n",
    "The dependent variable in this dataset is: survived(Categorical)\n",
    "\n",
    "The independent Continous variables are: age and fare. <br>\n",
    "The independent Categorical variables are: pclass, sex, sibsp, parch, embarked and alone.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e4a1d28e4fbd4505dab47665d95dcf80",
     "grade": false,
     "grade_id": "load_titanic",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "col_names = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
    "             'embarked', 'alone', 'survived']\n",
    "\n",
    "titanic_data = sns.load_dataset(\"titanic\")\n",
    "\n",
    "titanic_data = titanic_data[col_names]\n",
    "titanic_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "572443bec570e3e8c48b6422d65eeac9",
     "grade": false,
     "grade_id": "problem_1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "Write a function for data preprocessing which will involve removing NaNs and converting categorical features into corresponding binarized features. Function should return a dataframe which should contain binarized features as well as numerical features along with the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "54af1149c28739e4dd537dbdc0baeb1c",
     "grade": false,
     "grade_id": "problem_1_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    '''\n",
    "    1. Preprocess data by removing all NaNs. \n",
    "    2. Create binarized features dataframe for each of the categorical features(can use get_dummies() function)\n",
    "    3. Construct a dataframe for numerical feature\n",
    "    4. Combine two DataFrames into a new DataFrame\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe containing titanic dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a dataframe \n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a23711f1f0cc3d48f7fa3cb86d7bac43",
     "grade": false,
     "grade_id": "sample_output_heading",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Sample output of the 1st 5 rows for the correct solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b36e284373ddce97f28ab13b281cb467",
     "grade": false,
     "grade_id": "sample_output",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = preprocess_data(titanic_data)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7ba0c4a0100e92c59546be58cbd08582",
     "grade": true,
     "grade_id": "problem_1_tests",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(len(data.index), 712)\n",
    "assert_equal(len(data.columns), 12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2ef856f15221faaeabcf30fc6b6d2f5b",
     "grade": false,
     "grade_id": "problem_2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Problem 2\n",
    "\n",
    "We will now be using the above Titanic dataset to create a Decision tree model in order to predict whether a person survived(which is represented by binary 1) or died(which is represented by binary 0) based on the parameters provided in the data set.\n",
    "\n",
    "In the code cell below do the following:\n",
    "\n",
    "- Create a function for creating a Decision tree Classification model using sci-kit learn keeping random_state=40 and maximum depth as a parameter.<br>\n",
    "- Fit your model on the training features and labels (which are stored in d_train and l_train).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "35deafefeb1f5d4f83b6ad9927afd022",
     "grade": false,
     "grade_id": "problem_2_variables",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "independent_vars = list(data)\n",
    "independent_vars.remove('survived')\n",
    "dependent_var = 'survived'\n",
    "\n",
    "frac = 0.3\n",
    "d_train, d_test, l_train, l_test = \\\n",
    "    train_test_split(data[independent_vars], data[dependent_var],\n",
    "                    test_size=frac, random_state=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "88a83128b491b3719fcfde02d6f8159d",
     "grade": false,
     "grade_id": "problem_2_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def DecisionTree(data,tree_depth):\n",
    "    '''\n",
    "    \n",
    "    Create a decision Tree Classification model using random_state=40 and max_depth as a parameters. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe containing titanic dataset\n",
    "    tree_depth: max depth of the tree\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A DecisionTree Classifier model \n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the depth of the tree increases, the chance of tree overfitting on training dataset increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f2a60f4fd702cdb4f49e2bbe10d6e366",
     "grade": false,
     "grade_id": "problem_2_calculations",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "titanic_model_3 = DecisionTree(data,tree_depth=3)\n",
    "titanic_model_8 = DecisionTree(data,tree_depth=8)\n",
    "\n",
    "predicted_3 = titanic_model_3.predict(d_test)\n",
    "score_test_3 = 100.0 * metrics.accuracy_score(l_test, predicted_3)\n",
    "print(f'Decision Tree Classification [Titanic Test Data(depth=3)] Score = {score_test_3:4.1f}%\\n')\n",
    "\n",
    "predicted_3 = titanic_model_3.predict(d_train)\n",
    "score_train_3 = 100.0 * metrics.accuracy_score(l_train, predicted_3)\n",
    "print(f'Decision Tree Classification [Titanic Train Data(depth=3)] Score = {score_train_3:4.1f}%\\n')\n",
    "\n",
    "predicted_8 = titanic_model_8.predict(d_test)\n",
    "score_test_8 = 100.0 * metrics.accuracy_score(l_test, predicted_8)\n",
    "print(f'Decision Tree Classification [Titanic Test Data(depth=8)] Score = {score_test_8:4.1f}%\\n')\n",
    "\n",
    "predicted_8 = titanic_model_8.predict(d_train)\n",
    "score_train_8 = 100.0 * metrics.accuracy_score(l_train, predicted_8)\n",
    "print(f'Decision Tree Classification [Titanic Train Data(depth=8)] Score = {score_train_8:4.1f}%\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf1429520bec91211b1546d187d2648c",
     "grade": true,
     "grade_id": "problem_2_tests",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(isinstance(titanic_model_3, DecisionTreeClassifier), True)\n",
    "assert_equal(titanic_model_3.max_depth, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "854eb469a68a4cd878541664cfb26de6",
     "grade": false,
     "grade_id": "problem_3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problem 3\n",
    "\n",
    "It is necessary to know what features gives us the most information or are the most important ones to create a model. Create a function to find the top 4 most important feature names along with the correspending value of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "c3f29de76885b694790857fab4cfd7af",
     "grade": false,
     "grade_id": "problem_3_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def importance(feature_names,model):\n",
    "    '''\n",
    "    Find top 4 most important feature names along with the correspending value of importance.  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_names : containing a list of independent variables(independent_vars)\n",
    "    model : model name for the decision tree regressor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A sorted list of 4 elements in decending order containing a tuple of feature and importance as an element\n",
    "    \n",
    "    Example of what return output will look like\n",
    "    --------------------------------------------\n",
    "    [('sex_female', 0.43425212116045792),\n",
    "     ('fare', 0.17077252568364368),\n",
    "     ('pclass', 0.15921755986638889),\n",
    "     ('age', 0.15394111874015437)]\n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    Use zip(feature_names, model.feature_importances_) and use reversed and sorted functions to get the zip in sorted order. \n",
    "    Then convert this into a list and take the first 4 elements. \n",
    "    Or if you are not using reversed, then return the last 4 elements.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "96a091d7644fac4ab5fceeb3988c7019",
     "grade": true,
     "grade_id": "problem_3_tests",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "imp = importance(independent_vars,titanic_model_3)\n",
    "a=[]\n",
    "for i in range(len(imp)):\n",
    "    a.append(np.round(imp[i][1],4))\n",
    "assert_almost_equal(a,[0.6597, 0.2367, 0.1036, 0.0],places=4)\n",
    "b=[]\n",
    "for i in range(len(imp)):\n",
    "    b.append(imp[i][0])\n",
    "\n",
    "assert_equal(b,['sex_female', 'pclass', 'age', 'embarked_S'])\n",
    "\n",
    "#Feature importance for titanic_model_3 will have the values : \n",
    "for i in range(len(imp)):\n",
    "    print(f'{imp[i][0]} importance = {100.0*imp[i][1]:5.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "42186a94297bc7a47e5c55fcf134e1fd",
     "grade": false,
     "grade_id": "problem_4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problem 4\n",
    "\n",
    "We will be using Hitters dataset which contains player information including his performance and salary for Major League Baseball from the 1986 and 1987 seasons. This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. \n",
    "\n",
    "We will try to predict the Salary of players based on the factors available to us(information like Number of Hits, Number of Home Runs, etc.) in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "53b723fb30442ff94aa421fad90889c1",
     "grade": false,
     "grade_id": "problem_4_read",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "wage = pd.read_csv('/home/data_scientist/data/misc/wages.csv')\n",
    "wage['log_Salary'] = np.log(wage['Salary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fa45fb9a49e681466e2dc5e9a106b851",
     "grade": false,
     "grade_id": "problem_4_matrix",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import patsy as pts \n",
    "\n",
    "y, x = pts.dmatrices('Salary + log_Salary ~ AtBat + Hits + HmRun + Runs + RBI + Walks + Years +' +\n",
    "                     'CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + C(League) +' + \n",
    "                     'C(Division) + PutOuts + Assists + Assists + C(NewLeague)' ,\n",
    "                     data=wage, return_type='dataframe')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "87697fcb1b2f6d51a69809e3cecc9975",
     "grade": false,
     "grade_id": "problem_4_que",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In the code cell below do the following:\n",
    "\n",
    "- Split data intro training:testing data set using random_state=23.\n",
    "- Create 2 Decision tree models for Salary and log_Salary repectively using sci-kit learn keeping random_state=23, mae as error criterion and maximum features as 7.<br>\n",
    "- Fit your model on the training features and labels (which are stored in ind_train and dep_train).\n",
    "- Find the RMSE values from both the models for Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "90219d26122cd7299ce75324e2e6d33a",
     "grade": false,
     "grade_id": "problem_4_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "- Split data intro training:testing data set(random_state=23)\n",
    "\n",
    "- Create following 2 Decision Tree Models with:\n",
    "\n",
    "1. a model named wage_model using Salary as dependent variable\n",
    "2. a model named log_wage_model using log_Salary as dependent variable\n",
    "\n",
    "- Using the above models, find :\n",
    "1. the rmse value using wage_model: rmse_wage\n",
    "2. the rmse value using log_wage_model: rmse_logwage\n",
    "\n",
    "'''\n",
    "frac = 0.3\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2605c410a8356ce2612e254fdb373c68",
     "grade": true,
     "grade_id": "problem_4_tests",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true(type(wage_model), type(DecisionTreeRegressor))\n",
    "assert_true(type(log_wage_model), type(DecisionTreeRegressor))\n",
    "assert_almost_equal(423.44, round(rmse_logwage,2), places = 3)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
