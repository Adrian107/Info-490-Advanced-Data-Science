{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "999922a2ae1fea8749505b14c1f62c64",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Week 2 Problem 1\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do not write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select *Kernel*, and restart the kernel and run all cells (*Restart & Run all*).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select *File* → *Save and CheckPoint*)\n",
    "\n",
    "5. When you are ready to submit your assignment, go to *Dashboard* → *Assignments* and click the *Submit* button. Your work is not submitted until you click *Submit*.\n",
    "\n",
    "6. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ce77b693b293a71e098ba4952f39349f",
     "grade": false,
     "grade_id": "due_date",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Due Date: 6 PM, January 29, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "92dd432f2c7bfab9dc638eb1e27c20f8",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from nose.tools import assert_equal, assert_true\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# We do this to ignore several specific warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Convenience function to plot confusion matrix\n",
    "\n",
    "# This method produces a colored heatmap that displays the relationship\n",
    "# between predicted and actual types from a machine leanring method.\n",
    "\n",
    "def confusion(test, predict, names, bins=3, title='Confusion Matrix'):\n",
    "\n",
    "    # Make a 2D histogram from the test and result arrays\n",
    "    pts, xe, ye = np.histogram2d(test, predict, bins)\n",
    "\n",
    "    # For simplicity we create a new DataFrame\n",
    "    pd_pts = pd.DataFrame(pts.astype(int), index=names, columns=names )\n",
    "    \n",
    "    # Display heatmap and add decorations\n",
    "    sns.set(font_scale=1.5)\n",
    "    hm = sns.heatmap(pd_pts, annot=True, fmt=\"d\")\n",
    "    \n",
    "    sns.set(font_scale=2.0)\n",
    "    hm.axes.set_title(title)\n",
    "    hm.axes.set_xlabel('Predicted')\n",
    "    hm.axes.set_ylabel('Actual')\n",
    "    sns.set(font_scale=1.0)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Dataset\n",
    "For this assignment we will be using the built-in dataset about breast cancer and the respective information on indivudal breast cancer cases. This dataset has 569 samples and a dimensionality size of 30. In this assignment, we will be using the various attributes and Logistic Regression in order to create a model that will predict whether the individual case is either malignant (harmful) or benign (non-harmful). Throughout the assignment, we will be improving our model from one that is very naïve to a more complicated one that accounts for all the attributes in the given dataset. \n",
    "\n",
    "The following code below imports the dataset as a pandas dataframe and previews a few sample data points. It also concatenates a column called classification which contains whether the record was determined to be a malignant or benign tumor. **Note: In this dataset, a malignant tumor has a value of 0 and a benign tumor has a value of 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f5e9c587c758d5931661898caae88f5e",
     "grade": false,
     "grade_id": "data_set",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension       ...        worst texture  worst perimeter  \\\n",
       "0                 0.07871       ...                17.33           184.60   \n",
       "1                 0.05667       ...                23.41           158.80   \n",
       "2                 0.05999       ...                25.53           152.50   \n",
       "3                 0.09744       ...                26.50            98.87   \n",
       "4                 0.05883       ...                16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \\\n",
       "0                0.2654          0.4601                  0.11890   \n",
       "1                0.1860          0.2750                  0.08902   \n",
       "2                0.2430          0.3613                  0.08758   \n",
       "3                0.2575          0.6638                  0.17300   \n",
       "4                0.1625          0.2364                  0.07678   \n",
       "\n",
       "   classification  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "NOTE: Make sure to load this data set before completing the assignment\n",
    "'''\n",
    "# Load in the dataset as a Pandas DataFrame\n",
    "data = load_breast_cancer()\n",
    "data_df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Preview the first few lines\n",
    "data_df['classification'] = data.target\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Naïve Implementation\n",
    "\n",
    "One very simple way to use binnarize records is to use a pre-determined cutoff value for a particular feature and classify anything less than a cutoff value as 0 and anything over the cutoff value as 1. The pre-determined cutoff value can be determined through other statistical methods to be general enough in order to correctly classify some instances of the data. However, it is very clear that this approach will not work for all cases and is very rudimentary. In the question below, complete the function `classify_cutoff` that takes in 3 parameters: `data_df` (the dataframe), `column_name` and `cutoff` and create a new column called `classification_simple` that contains either a 0 or 1 based on the cutoff value and the column name. If the column for that particular record is **less** than the cutoff value, then record a 0 in the `classification_simple` column; otherwise record a 1. \n",
    "\n",
    "For example, given the random 5 data points in the above code cell, if the `column_name` was mean radius and the `cutoff` was 12, then the 1st, 2nd, and 4th records would contain a 1 in the `classification_simple` column since the mean radius of these records are all greater than 12, and the 3rd and 5th records would be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "bba01d7722a9f2359f3bb51721de6249",
     "grade": false,
     "grade_id": "problem1_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def classify_cutoff(data_df, column_name, cutoff):\n",
    "    '''\n",
    "    Converts one column in Pandas.DataFrame data_df into binary\n",
    "    as a new column, and returns the new DataFrame (dat_df plus new binary column).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: A Pandas.DataFrame.\n",
    "    column: A string.\n",
    "    cutoff: An int.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A Pandas.DataFrame.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    data_df['classification_simple'] = 0\n",
    "    for i in range(data_df.shape[0]):\n",
    "        if data_df[column_name][i] <= cutoff:\n",
    "            data_df['classification_simple'][i] = 0\n",
    "        else:\n",
    "            data_df['classification_simple'][i] = 1\n",
    "    \n",
    "    return data_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1de49705541587381c62d02f84d46e2a",
     "grade": true,
     "grade_id": "problem1_test",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true(isinstance(classify_cutoff(data_df, 'mean radius', 2), pd.DataFrame))\n",
    "assert_true('classification_simple' in classify_cutoff(data_df, 'mean radius', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b836d4103fc541259d4c202dbc05415c",
     "grade": false,
     "grade_id": "cell-8db445bc12b6af0e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate the Confusion Matrix for the classify_cutoff (naïve implementation)\n",
    "# Should have a lot of Type I and Type II errors\n",
    "# We shall now use our Logistic Regression Models to reduce these errors\n",
    "\n",
    "new_data_df = classify_cutoff(data_df, 'mean radius', 15)\n",
    "confusion(new_data_df['classification'], \n",
    "             new_data_df['classification_simple'], ['Malignant', 'Benign'], 2, 'Breast Cancer Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - SGD Classifier\n",
    "\n",
    "In this question, we will be improving upon our very naïve implementation from earlier by actually using Logistic Regression. One way that you have learned to do this is through Stochastic Gradient Descent (SGD) that minimizes the loss function. Complete the following function below to use the `SGDClassifier` constructor using 'log' as the value to the loss parameter to the contructor. Here are the function specifications:\n",
    "\n",
    "- 'log' as the value to the loss parameter to the SGDClassifier constructor\n",
    "- use the `train_test_split` method to split the features and labels parameters into testing and training data\n",
    "- use a value of 23 for the random state\n",
    "- use the labels as the stratify parameter\n",
    "- use a test size of 0.3 into the `train_test_split` method\n",
    "- return the predicted values after the `predict()` function runs and the dependent test data from the `train_test_split` function as a 2-tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "9f07f3670fdbd6d852726f0c0e144068",
     "grade": false,
     "grade_id": "problem2_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sgd_classifier(features, labels):\n",
    "    '''\n",
    "    Return the predicted data using the SGDClassifier technique\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    features: A Pandas dataframe representing the features data\n",
    "    labels: A Pandas dataframe represengint the labels data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    predicted, y_test: A 2-tuple consisting of 2 respective numpy arrays\n",
    "        - predicted is the result of the predict() function\n",
    "        - y_test is the data that can be used to check how accurate the model was\n",
    "    '''\n",
    "    \n",
    "    model = LogisticRegression(C=1E6)\n",
    "    # YOUR CODE HERE\n",
    "    sgd_model = SGDClassifier(loss='log')\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3, stratify = labels, random_state = 23)\n",
    "    sgd_model = model.fit(x_train, y_train)\n",
    "    predicted = sgd_model.predict(x_test)\n",
    "    \n",
    "    return predicted, y_test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5025d8e9d082e840bf73d98daea4164b",
     "grade": true,
     "grade_id": "problem2_test",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true(isinstance(sgd_classifier(data.data, data.target)[0], np.ndarray))\n",
    "assert_true(isinstance(sgd_classifier(data.data, data.target)[1], np.ndarray))\n",
    "assert_equal(len(sgd_classifier(data.data, data.target)[0]), int(math.ceil(len(data.data)*0.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b888f7a7fb7a0f42c3b9ce35ba387d4a",
     "grade": false,
     "grade_id": "cell-c705e6fb1f02532e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate the Confusion Matrix using the SGD Classifier\n",
    "# Should have drastically reduced the number of Type I and Type II errors from our naïve implementation\n",
    "\n",
    "predicted, y_test = sgd_classifier(data.data, data.target)\n",
    "confusion(y_test.reshape(y_test.shape[0]), predicted, ['Malignant', 'Benign'], 2, 'Breast Cancer Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 - LogisticRegression\n",
    "\n",
    "In this question, we will be using the standard LogisticRegression constructor to perform logistic regression instead of explicitly using the SGD Classifier. Complete the following function below to use the `LogisticRegression` constructor. Here are the function specifications:\n",
    "\n",
    "- use the `train_test_split` method to split the features and labels parameters into testing and training data\n",
    "- use a value of 23 for the random state\n",
    "- use the labels as the stratify parameter\n",
    "- use a test size of 0.3 into the `train_test_split` method\n",
    "- return the predicted values after the `predict()` function runs and the dependent test data from the `train_test_split` function as a 2-tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "4740f632b72da83f78b6256858445614",
     "grade": false,
     "grade_id": "problem3_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def logistic_regression(features, labels):\n",
    "    '''\n",
    "    Return the predicted data using the LogisticRegression technique technique\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    features: A Pandas dataframe representing the features data\n",
    "    labels: A Pandas dataframe represengint the labels data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    predicted, y_test: A 2-tuple consisting of 2 respective numpy arrays\n",
    "        - predicted is the result of the predict() function\n",
    "        - y_test is the data that can be used to check how accurate the model was\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    model = LogisticRegression(C=1E6)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3, stratify = labels, random_state = 23)\n",
    "    lr_model = model.fit(x_train, y_train)\n",
    "    predicted = lr_model.predict(x_test)\n",
    "    \n",
    "    return predicted, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2898f99d859c4f14adf4580ad7000027",
     "grade": true,
     "grade_id": "problem3_test",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true(isinstance(logistic_regression(data.data, data.target)[0], np.ndarray))\n",
    "assert_true(isinstance(logistic_regression(data.data, data.target)[1], np.ndarray))\n",
    "assert_equal(len(logistic_regression(data.data, data.target)[0]), int(math.ceil(len(data.data)*0.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "70b9fb6e87d22c8b2667ccfe8d2a66c0",
     "grade": false,
     "grade_id": "cell-e26789914f06a281",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate the Confusion Matrix using the Logistic Regression Method\n",
    "# We should achieve very similar statistics to our SGDClassifier model\n",
    "\n",
    "predicted, y_test = logistic_regression(data.data, data.target)\n",
    "confusion(y_test.reshape(y_test.shape[0]), predicted, ['Malignant', 'Benign'], 2, 'Breast Cancer Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
